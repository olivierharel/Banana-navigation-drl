{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training report\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n",
    "5 types of agents were trained:\n",
    "- DQN: Vanilla Deep-Q-Network\n",
    "- DDQN: double DQN leveraging target Q network to evaluate the Q value) during training\n",
    "- Prioritized: DQN trained with prioritized experience replay during training\n",
    "- Dueling: Dueling Networks DQN\n",
    "- DDQN+Prioritized+Dueling: combines Double DQN/Priotized replay/Dueling networks\n",
    "\n",
    "They all share the same underlying DL model, with the exception of the dueling network DQN (and the combined network).\n",
    "However the dueling networks are designed so that their total number of weights is similar to that of the single network used in (DQN, DDQN, Priorized). \n",
    "\n",
    "The DQN network is simple with 2 fully connected hidden layer, each with 64 nodes. I didn't try different networks as this configuration worked well and its limited size appears adequate for the limited dimensions of the environment (37 states, 4 actions).\n",
    "\n",
    "All agents in general share the same hyperparameters. An epsilon greedy policy is used during training with a decay of 0.995 consistent with the target 1000 training epidodes (0.995 ^ 100 = 0.6, 0.995 ^ 1000 < 0.01).\n",
    "\n",
    "The agents are compared, score wise, during training and in testing.\n",
    "A higher than required early termination (sucess) threshold of 17 is used. All agents reach the minimum required score of 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "#### Training\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>Agent</td>\n",
    "<td>Plot</td>\n",
    "<td>Notes</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>DQN</td>\n",
    "<td><img src=\"./Report_images/dqn.png\" alt=\"DQN score v/s episode\" /></td>\n",
    "<td>Score >= 13 cleared around episode 250-300. Did not reach score of 17 (in several trainings) and seems to plateau around 15</td>\n",
    "</tr>   \n",
    "<tr>\n",
    "<td>DDQN</td>\n",
    "<td><img src=\"./Report_images/double-dqn.png\" alt=\"DDQN score v/s episode\" /></td>\n",
    "<td>Reaches higher overall scores than DQN but didn't reach 17 (in several trainings) and also seems to be reaching a plateau around 16</td>\n",
    "</tr>   \n",
    "<tr>\n",
    "<td>Prioritized Replay</td>\n",
    "<td><img src=\"./Report_images/prioritized.png\" alt=\"Prioritized replay score v/s episode\" /></td>\n",
    "<td>Reached consistently 17 in several trainings (typically around episodes 800-900. Could benefit from more episodes as the score curve trend is still rising. In general the best of the single-improvement networks</td>\n",
    "</tr>   \n",
    "<tr>\n",
    "<td>Dueling Networks</td>\n",
    "<td><img src=\"./Report_images/dueling.png\" alt=\"Dueling DQN score v/s episode\" /></td>\n",
    "<td>Reaches scores >=16, but typically did not hit 17 in 1000 episides. The score curve trends seems to still be rising slightly. Increasing the number of episode could allow this agent to reach 17</td>\n",
    "</tr>   \n",
    "<tr>\n",
    "<td>DDQN+Prioritized+Dueling </td>\n",
    "<td><img src=\"./Report_images/combined.png\" alt=\"DDQN+Prioritized+Dueling score v/s episode\" /></td>\n",
    "<td>Typically reaches a score of 17 within 1000 episodes. While it doesn't reach higher scores than the simple prioritized replay agent, the combined agent shows the best score growth in the initial phase of training and its trend also shows the possibly to increase further the training score</td>\n",
    "</tr>   \n",
    "</table>\n",
    "\n",
    "#### Testing\n",
    "<table>\n",
    "<tr>\n",
    "<td>Agent</td>\n",
    "<td>Mean score</td>\n",
    "<td>median</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>DQN </td>\n",
    "<td>15.28</td>\n",
    "<td>16.5</td>\n",
    "</tr>   \n",
    "<tr>\n",
    "<td>DDQN </td>\n",
    "<td>16.81</td>\n",
    "<td>17.0</td>\n",
    "</tr>    \n",
    "<tr>\n",
    "<td>Prioritized replay </td>\n",
    "<td>17.5</td>\n",
    "<td>18.0</td>\n",
    "</tr>    \n",
    "<tr>\n",
    "<td>Dueling networks </td>\n",
    "<td>16.75</td>\n",
    "<td>17.0</td>\n",
    "</tr>    \n",
    "<tr>\n",
    "<td>DDQN+Prioritized+Dueling </td>\n",
    "<td>16.84</td>\n",
    "<td>16.0</td>\n",
    "</tr>    \n",
    "</table>\n",
    "\n",
    "The testing scores generally match the observations from training with prioritized replay yielding the best average and median scores.\n",
    "\n",
    "The combined agent (DDQN+Prioritized+Dueling) doesn't improve much on any of its individual variations. Its median score is actually lower than the simple DQN network. The findings from the 'rainbow' paper don't quite apply for this environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work ideas\n",
    "\n",
    "* Try more agent variants including the options outlined in the 'rainbow' paper (https://arxiv.org/abs/1710.02298).\n",
    "* Shmoo hyperparameters, trade-off complexity (network size) v/s highest score\n",
    "* Defined and train agents which state observation is given by the 2D image, as seen during the game, rather than by processed data provided by the game environment (rays...). The DL network would include convolution layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
